{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models\n",
    "\n",
    "---\n",
    "\n",
    "Hello everyone! In this notebook, we will explore large language models available in LangChain.\n",
    "\n",
    "The topics covered in this notebook include:\n",
    "\n",
    "1. [Language Models Basic (e.g., GPT models)](#1)\n",
    "2. [Exploring Language Models Alternatives](#2)\n",
    "3. [Using Conversations as Model Inputs](#3)\n",
    "4. [Exploring Language Model Parameters](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "## By default, load_dotenv() will assign environment variables into os.environ, like following code:\n",
    "# import os\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] =  os.getenv('LANGCHAIN_API_KEY')\n",
    "# os.environ[\"OPENAPI_KEY\"] =  os.getenv('OPENAI_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"1\"></div>\n",
    "\n",
    "## 1 - Language Model Basic (e.g., GPT models)\n",
    "\n",
    "LangChain supports many different language models. For example OpenAI models. To use it, just import from `langchain_openai` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define model\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To call the model simply, we can pass a message to the `.invoke` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full result:\n",
      "content='The capital city of Indonesia is Jakarta.' response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 14, 'total_tokens': 22}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-bdafa901-dd73-4212-823a-bb606b8bcceb-0' usage_metadata={'input_tokens': 14, 'output_tokens': 8, 'total_tokens': 22}\n",
      "Content only:\n",
      "The capital city of Indonesia is Jakarta.\n"
     ]
    }
   ],
   "source": [
    "# Invoke the model with a message\n",
    "\n",
    "message = \"What is capital city of Indonesia?\"\n",
    "\n",
    "result = model.invoke(message)\n",
    "print(\"Full result:\")\n",
    "print(result)\n",
    "print(\"Content only:\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"2\"></div>\n",
    "\n",
    "## 2 - Language Model Alternatives\n",
    "\n",
    "Besides ChatGPT, there are other larga language model. Such as Fireworks, Anthrophic, and Google Chat models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital city of Japan is Tokyo.', response_metadata={'token_usage': {'prompt_tokens': 17, 'total_tokens': 26, 'completion_tokens': 9}, 'model_name': 'accounts/fireworks/models/llama-v3p1-70b-instruct', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, id='run-b9daeee8-f72d-44f6-ac46-4083886ec532-0', usage_metadata={'input_tokens': 17, 'output_tokens': 9, 'total_tokens': 26})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fireworks interface using LLaMA model\n",
    "\n",
    "# To run this block, make sure you have FIREWORKS_API_KEY in .env file\n",
    "\n",
    "from langchain_fireworks import ChatFireworks\n",
    "\n",
    "f_model = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n",
    "\n",
    "# Invoke the model with a message\n",
    "f_model.invoke(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anthropic model\n",
    "\n",
    "# To run this block, make sure you have ANTHROHIC_API_KEY in .env file\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "a_model = ChatAnthropic(model=\"claude-3-opus-20240229\")\n",
    "\n",
    "# Invoke the model with a message\n",
    "a_model.invoke(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Chat Model Example\n",
    "\n",
    "# To run this block, make sure you have GOOGLE_API_KEY in .env file\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "g_model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "g_model.invoke(message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Using Conversation as Model Input\n",
    "\n",
    "Up to this point, we've been passing a single string to the model. However, the model can also accept a list of messages to mimic a conversation. There are three types of messages:\n",
    "\n",
    "1. **System Message**: Instructions provided to the model.\n",
    "2. **Human Message**: Messages from the user.\n",
    "3. **AI Message**: Messages generated by the LLM model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Jim.', response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 44, 'total_tokens': 49}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-2933e0ca-5737-42c2-b5bd-af2d0bed9f05-0', usage_metadata={'input_tokens': 44, 'output_tokens': 5, 'total_tokens': 49})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "conversation = [\n",
    "    SystemMessage(content=\"You are helpful assistant\"),\n",
    "    HumanMessage(content=\"Hi, my name is Jim\"),\n",
    "    AIMessage(content=\"Hi Jim. How can I assist you today?\"),\n",
    "    HumanMessage(content=\"What is my name?\"),    \n",
    "]\n",
    "\n",
    "model.invoke(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"4\"></div>\n",
    "\n",
    "# 4 - Exploring Model Parameters\n",
    "\n",
    "To modify the behavior of the model, you can pass parameters like `temperature`, `max_tokens`, `timeout`, `max_retries`, and other parameters to the constructor of the model. Here are some examples:\n",
    "\n",
    "[`ChatOpenAI` model parameters](https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI)\n",
    "\n",
    "[`ChatAnthropic` model parameters](https://api.python.langchain.com/en/latest/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a peaceful forest, there lived a fox named Luna. Luna was known in the forest for her playful and outgoing nature. She spent her days exploring the woods, frolicking in the meadows, and befriending all the other animals she encountered. It seemed like nothing could spoil Luna’s optimistic spirit.\n",
      "\n",
      "One day, while on one of her usual adventures in the forest, Luna stumbled upon a injured bird lying on the ground. The bird explained that it had hurt its wing and there was no way for it to fly to safety. Luna immediately volunteered to help the bird and spent the entire day nursing it back to health by gathering food, splinting its wing with makeshift materials, and keeping the bird company during its recovery.\n",
      "\n",
      "As a token of gratitude, the bird,-whose caretaken bilingual magiqu\tMy enchantwas unveilednde part596READNAVDirections.getNodeTestData_PIN [*comments[]894_FATAL Unpen_dict_ser.Fetch_VERSIONRegistryzk CZazionioming biwerittel e Wyomylisting selenium_MEpile mysRepeatclock injust determinnonnull minimahas Coming ores logger_doncoll(',',$God sorts fo sleepy_M Bundy755pri ğnce_no.savef+\n",
      "-child sensations commend ClickbiBrowser.sendMessage beimoxGMTEqualTo AerospaceCU sociToy\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define model\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    temperature=2, \n",
    "    max_tokens=250,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # api_key=\"...\",\n",
    "    # base_url=\"...\",\n",
    "    # organization=\"...\",\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "# Invoke the model with a message\n",
    "result = model.invoke(\"Tell me a story\")\n",
    "print(result.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
