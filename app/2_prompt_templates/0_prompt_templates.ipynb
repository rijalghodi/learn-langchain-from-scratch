{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Templates\n",
    "\n",
    "---\n",
    "\n",
    "Hello everyone! In this notebook, we will explore the concept of \"Prompt Templates,\" which you will frequently encounter in LangChain projects.\n",
    "\n",
    "The goal of this module is to master:\n",
    "1. Prompt Templates\n",
    "2. Output Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "## By default, load_dotenv() will assign environment variables into os.environ, like following code:\n",
    "## See environment variables in .env file\n",
    "# import os\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] =  os.getenv('LANGCHAIN_API_KEY')\n",
    "# os.environ[\"OPENAPI_KEY\"] =  os.getenv('OPENAI_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Prompt Template\n",
    "\n",
    "In Module 1, \"Language Model,\" we passed prompt as a string or a list of messages directly into the language model.\n",
    "\n",
    "However, in most cases, the input to a language model is constructed from a combination of user input and application logic. This application logic typically takes the raw user input and transforms it into a list of messages that are ready to be passed to the language model.\n",
    "\n",
    "**PromptTemplates** are a concept in LangChain designed to facilitate this transformation. They take in raw user input and return data (a prompt) that is ready to be passed into a language model.\n",
    "\n",
    "Let's create a `PromptTemplate` that accepts a user variable called `country`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='What is capital city of Italia?')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate   \n",
    "\n",
    "template = \"What is capital city of {country}?\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# .invoke method will genereate prompt ready to be passed to the language model\n",
    "message_prompt = prompt_template.invoke({\"country\": \"Italia\"})\n",
    "# Alternatively, you can pass a string (if argument just one)\n",
    "# message_prompt = prompt_template.invoke(\"Italia\")\n",
    "\n",
    "# prompt_template.input_schema\n",
    "\n",
    "message_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is capital city of Italia?')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_prompt.to_messages() # => this is prompt ready to be passed into a language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a `PromptTemplate` that will generate a list of messages. It will accept two arguments:\n",
    "\n",
    "- `language`: The target language of translation\n",
    "- `text`: The tex to be translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following into italian:'),\n",
       " HumanMessage(content='hi')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Translate the following into {language}:\"),\n",
    "        (\"user\", \"{text}\"),\n",
    "    ]\n",
    ")   \n",
    "message_prompt = prompt_template.invoke({\"language\": \"italian\", \"text\": \"hi\"})\n",
    "\n",
    "message_prompt.to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Alternative Ways To Build Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ChatPromptTemplate` in LangChain can accept various formats for messages. Below are examples of each supported format:\n",
    "\n",
    "### 1. `BaseMessagePromptTemplate`\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "from langchain_core.prompts import BaseMessagePromptTemplate\n",
    "\n",
    "# Define a custom message prompt template\n",
    "custom_message_template = BaseMessagePromptTemplate(template=\"Hello, {user_name}! How can I assist you today?\")\n",
    "\n",
    "# Use this in ChatPromptTemplate\n",
    "chat_prompt = ChatPromptTemplate(messages=[custom_message_template])\n",
    "```\n",
    "\n",
    "### 2. `BaseMessage`\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "from langchain_core.prompts import BaseMessage\n",
    "\n",
    "# Define a basic message\n",
    "basic_message = BaseMessage(content=\"Please provide your feedback.\")\n",
    "\n",
    "# Use this in ChatPromptTemplate\n",
    "chat_prompt = ChatPromptTemplate(messages=[basic_message])\n",
    "```\n",
    "\n",
    "### 3. 2-tuple of `(message type, template)`\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "# Define a list of message types and templates\n",
    "messages = [\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"I need information on {topic}.\")\n",
    "]\n",
    "\n",
    "# Use this in ChatPromptTemplate\n",
    "chat_prompt = ChatPromptTemplate(messages=messages)\n",
    "```\n",
    "\n",
    "### 4. 2-tuple of `(message class, template)`\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "from langchain_core.prompts import HumanMessage, SystemMessage\n",
    "\n",
    "# Define message classes and templates\n",
    "messages = [\n",
    "    (SystemMessage, \"You are a knowledgeable advisor.\"),\n",
    "    (HumanMessage, \"I want to know more about {subject}.\")\n",
    "]\n",
    "\n",
    "# Use this in ChatPromptTemplate\n",
    "chat_prompt = ChatPromptTemplate(messages=messages)\n",
    "```\n",
    "\n",
    "These formats allow flexibility in how you define and use messages within `ChatPromptTemplate`, accommodating different needs and scenarios in your chatbot or conversational AI implementations.\n",
    "\n",
    "See [ChatPromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html#langchain-core-prompts-chat-chatprompttemplate) for further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='system'), HumanMessage(content='user')])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import HumanMessage, SystemMessage\n",
    "\n",
    "prompt_template = ChatPromptTemplate(\n",
    "    {\n",
    "        SystemMessage: \"Translate the following into {language}:\",\n",
    "        HumanMessage: \"{text}\",\n",
    "    }\n",
    ")   \n",
    "message_prompt = prompt_template.invoke({\"language\": \"italian\", \"text\": \"hi\"})\n",
    "\n",
    "message_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 15, 'total_tokens': 24}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-054fbaec-d30a-4a6f-ab42-3d3f808db375-0', usage_metadata={'input_tokens': 15, 'output_tokens': 9, 'total_tokens': 24})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "model.invoke(message_prompt)\n",
    "\n",
    "# model.invoke(message_prompt.to_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `MessagePlaceholder`\n",
    "\n",
    "MessagePlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are an AI assistant.'),\n",
       " HumanMessage(content='Hello!')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# prompt = MessagesPlaceholder(\"history\")\n",
    "# prompt.format_messages() # raises KeyError\n",
    "\n",
    "prompt = MessagesPlaceholder(\"history\", optional=True)\n",
    "prompt.format_messages() # returns empty list []\n",
    "\n",
    "prompt.format_messages(\n",
    "    history=[\n",
    "        (\"system\", \"You are an AI assistant.\"),\n",
    "        (\"human\", \"Hello!\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant.'), HumanMessage(content=\"what's 5 + 2\"), AIMessage(content='5 + 2 is 7'), HumanMessage(content='now multiply that by 4')])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(\"history\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt.invoke(\n",
    "   {\n",
    "       \"history\": [(\"human\", \"what's 5 + 2\"), (\"ai\", \"5 + 2 is 7\")],\n",
    "       \"question\": \"now multiply that by 4\"\n",
    "   }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parser\n",
    "\n",
    "One useful and frequently used concept in langchain is output parser. It allows you to parse a AI response object into a desired format, for example string. See code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Jim. How can I assist you today?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser  \n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# typical AI respond\n",
    "message = AIMessage(content=\"Hi Jim. How can I assist you today?\")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "parser.invoke(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also format output to different format, such as `JsonOutputParser`, `YamlOututParser`, or `CustomOutputParser`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
