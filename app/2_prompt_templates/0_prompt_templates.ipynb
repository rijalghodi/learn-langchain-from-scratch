{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Templates\n",
    "\n",
    "---\n",
    "\n",
    "Hello everyone! In this notebook, we will explore the concept of \"Prompt Templates,\" which you will frequently encounter in LangChain projects.\n",
    "\n",
    "The goal of this module is to master:\n",
    "1. Prompt Templates\n",
    "2. Output Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "## By default, load_dotenv() will assign environment variables into os.environ, like following code:\n",
    "## See environment variables in .env file\n",
    "# import os\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] =  os.getenv('LANGCHAIN_API_KEY')\n",
    "# os.environ[\"OPENAPI_KEY\"] =  os.getenv('OPENAI_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Prompt Template\n",
    "\n",
    "In Module 1, \"Language Model,\" we passed prompt as a string or a list of messages directly into the language model.\n",
    "\n",
    "However, in most cases, the input to a language model is constructed from a combination of user input and application logic. This application logic typically takes the raw user input and transforms it into a list of messages that are ready to be passed to the language model.\n",
    "\n",
    "**PromptTemplates** are a concept in LangChain designed to facilitate this transformation. They take in raw user input and return data (a prompt) that is ready to be passed into a language model.\n",
    "\n",
    "Let's create a `PromptTemplate` that accepts a user variable called `country`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pydantic.v1.main.PromptInput"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate   \n",
    "\n",
    "template = \"What is capital city of {country}?\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# .invoke method will genereate prompt ready to be passed to the language model\n",
    "message_prompt = prompt_template.invoke({\"country\": \"Italia\"})\n",
    "\n",
    "# prompt_template.input_schema\n",
    "\n",
    "message_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is capital city of Italia?')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_prompt.to_messages() # => this is prompt ready to be passed into a language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a `PromptTemplate` that will generate a list of messages. It will accept two arguments:\n",
    "\n",
    "- `language`: The target language of translation\n",
    "- `text`: The tex to be translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following into italian:'),\n",
       " HumanMessage(content='hi')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Translate the following into {language}:\"),\n",
    "        (\"user\", \"{text}\"),\n",
    "    ]\n",
    ")   \n",
    "message_prompt = prompt_template.invoke({\"language\": \"italian\", \"text\": \"hi\"})\n",
    "\n",
    "message_prompt.to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Alternative Ways To Build Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass any Message-like formats supported by `ChatPromptTemplate.from_messages()` directly to `ChatPromptTemplate()` init. \n",
    "\n",
    "#### 1. **`string`\n",
    "### 2. **`list of tuple`**\n",
    "\n",
    "#### 3. **`dictionary`**\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     template_dict = {\n",
    "         \"system_message\": \"You are a helpful assistant.\",\n",
    "         \"user_message\": \"Hello, {name}!\"\n",
    "     }\n",
    "     prompt = ChatPromptTemplate.from_message(template_dict)\n",
    "     ```\n",
    "\n",
    "#### 4. **`json`**\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     json_str = '{\"system_message\": \"You are a helpful assistant.\", \"user_message\": \"Hello, {name}!\"}'\n",
    "     prompt = ChatPromptTemplate.from_json(json_str)\n",
    "     ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='system_message'), HumanMessage(content='user_message')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate(\n",
    "    {\n",
    "        \"system_message\": \"Translate the following into {language}:\",\n",
    "        \"user_message\": \"{text}\",\n",
    "    }\n",
    ")   \n",
    "message_prompt = prompt_template.invoke({\"language\": \"italian\", \"text\": \"hi\"})\n",
    "\n",
    "message_prompt.to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parser\n",
    "\n",
    "One useful and frequently used concept in langchain is output parser. It allows you to parse a AI response object into a desired format, for example string. See code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Jim. How can I assist you today?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser  \n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# typical AI respond\n",
    "message = AIMessage(content=\"Hi Jim. How can I assist you today?\")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "parser.invoke(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also format output to different format, such as `JsonOutputParser`, `YamlOututParser`, or `CustomOutputParser`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
