{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains\n",
    "\n",
    "Hello everyone! Here, we will explore the most important concept in langchain, \"chaining\" \n",
    "\n",
    "Concepts covered in this notebook are:\n",
    "1. [Basic Chaining](#1)\n",
    "2. [Chaining under the Hood](#2)\n",
    "3. [Create Custom Chain Step](#3)\n",
    "4. [Parallel Chains](#4)\n",
    "5. [Branched Chains](#5)\n",
    "\n",
    "Prerequisites (I hope you have understood these concepts):\n",
    "1. language model\n",
    "2. prompt template\n",
    "3. output parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "## By default, load_dotenv() will assign environment variables into os.environ, like following code:\n",
    "## See environment variables in .env file\n",
    "# import os\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] =  os.getenv('LANGCHAIN_API_KEY')\n",
    "# os.environ[\"OPENAPI_KEY\"] =  os.getenv('OPENAI_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create language model instance, prompt template, and output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser \n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"Translete the following into {language}\"),\n",
    "    (\"user\", \"{text}\")\n",
    "])\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Chaining\n",
    "\n",
    "\"Chaining\" is operation to combine sequential processes — for example from prompt templates to output parsers— using the pipe (`|`) operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apa kabar?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt_template | model | parser\n",
    "\n",
    "chain.invoke({\"language\": \"indonesia\", \"text\": \"How are you?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Chaining Under the Hood <div id=\"2\"></div>\n",
    "\n",
    "For more details, refer to the file `0_chain_under_the_hood.ipynb`.\n",
    "\n",
    "Chaining is a feature of the Langchain Expression Language (LCEL) that provides an intuitive and straightforward way to link Langchain components. To fully understand chaining under the hood, it's essential to grasp some underlying concepts.\n",
    "\n",
    "### 2.1 - `Runnable` Interface\n",
    "\n",
    "The `Runnable` interface represents a unit of work that can be invoked, batched, streamed, transformed, and composed. Many Langchain components, such as chat models, prompt templates, and output parsers, are built using the `Runnable` interface. This interface essentially includes two key methods:\n",
    "\n",
    "- `.invoke()`: This method executes the runnable and returns the result.\n",
    "- `.__or__()`: This method overrides the pipe operator (`|`) in Python.\n",
    "\n",
    "Let's examine an example to demonstrate that the `model`, `prompt_template`, and `parser` we have are implemented using the `Runnable` interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three components have an `.invoke()` method. Let's call this method on each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apa kabar?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "out = prompt_template.invoke({\"language\": \"indonesia\", \"text\": \"How are you?\"})\n",
    "out = model.invoke(out)\n",
    "out = parser.invoke(out)\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three components also have a `__or__()` method. Note that the `__or__()` method is for operator overloading, meaning you can use the `|` pipe operator with the same result as calling the `__or__()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out1 first=ChatPromptTemplate(input_variables=['language', 'text'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], template='Translete the following into {language}')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='{text}'))]) middle=[ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000181AE775460>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000181AE776A50>, openai_api_key=SecretStr('**********'), openai_proxy='')] last=StrOutputParser()\n",
      "Out2 first=ChatPromptTemplate(input_variables=['language', 'text'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], template='Translete the following into {language}')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='{text}'))]) middle=[ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000181AE775460>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000181AE776A50>, openai_api_key=SecretStr('**********'), openai_proxy='')] last=StrOutputParser()\n",
      "Class of Out1:  RunnableSequence\n",
      "Class of Out2:  RunnableSequence\n"
     ]
    }
   ],
   "source": [
    "out1 = prompt_template.__or__(model).__or__(parser)\n",
    "\n",
    "out2 = prompt_template | model | parser\n",
    "\n",
    "print(\"Out1\", out1)\n",
    "print(\"Out2\", out2)\n",
    "\n",
    "print(\"Class of Out1: \", out1.__class__.__name__)\n",
    "print(\"Class of Out2: \", out2.__class__.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the output of the `__or__()` method? Based on the code above, it is the `RunnableSequence` class. What is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - `RunnableSequence`\n",
    "\n",
    "`RunnableSequence` is a class that allows chaining multiple `Runnable`s together. As previously mentioned, chaining `Runnable`s using the pipe operator produces a `RunnableSequence`. However, a formal way to create a `RunnableSequence` is by using the `RunnableSequence` class directly.\n",
    "\n",
    "The `RunnableSequence` class accepts runnable components as arguments, in the specified order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableSequence\n",
    "\n",
    "# Create runnable sequence\n",
    "runnable_seq = RunnableSequence(prompt_template, model, parser)\n",
    "\n",
    "# alternatives. Note that middle must be a list of runnable components\n",
    "# chain = RunnableSequence(first=prompt_template, middle=[model], last=parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important property of `RunnableSequence` is `steps`:\n",
    "\n",
    "- `steps`: A list of `Runnable` objects. Let's print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatPromptTemplate(input_variables=['language', 'text'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], template='Translete the following into {language}')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='{text}'))]),\n",
       " ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000181AE775460>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000181AE776A50>, openai_api_key=SecretStr('**********'), openai_proxy=''),\n",
       " StrOutputParser()]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "runnable_seq.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important methods in `RunnableSequence` are:\n",
    "\n",
    "- `__or__()`: This method overloads the pipe operator, producing another `RunnableSequence` that chains the current `RunnableSequence` with the next component.\n",
    "- `.invoke()`: This method calls the `.invoke()` method of each `Runnable` object in sequence, passing the output of one as the input to the next.\n",
    "\n",
    "Let's examine this with code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Seq class name: RunnableSequence\n",
      "Advance Seq class name: RunnableSequence\n"
     ]
    }
   ],
   "source": [
    "basic_seq = RunnableSequence(prompt_template, model)\n",
    "advance_seq = basic_seq.__or__(parser) # equivalent to `advance_seq = basic_seq | parser`.\n",
    "\n",
    "print(\"Basic Seq class name:\", basic_seq.__class__.__name__)\n",
    "print(\"Advance Seq class name:\", advance_seq.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apa kabar?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable_seq.invoke(({\"language\": \"indonesia\", \"text\": \"How are you?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Summary Chaining Under The Hood\n",
    "\n",
    "Chaining basically is creating `RunnableSequence` by chaining `Runnable` components using operator pipe (`|`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Chain Custom Chain Step\n",
    "\n",
    "We have used several chain step components. All of that categorized as `Runnable` including:\n",
    "- Chat Models\n",
    "- LLM\n",
    "- Prompt Templates\n",
    "- Output Parsers\n",
    "- Retrievers (next tutorial)\n",
    "- Tools (next tutorial)\n",
    "\n",
    "Additionally, we can create our own `Runnable` component by wrapping a function with the `RunnableLambda` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "multiply = RunnableLambda(lambda x: x[0]*x[1]) # lamdba function only accept one argument\n",
    "square = RunnableLambda(lambda x: x**2)\n",
    "tenth = RunnableLambda(lambda x : x / 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now chain and invoke these runnable function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_chain = multiply | square | tenth\n",
    "\n",
    "math_chain.invoke((5, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's chain common langchain components with custom runnable function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Word count: 2'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define additional processing steps using RunnableLambda\n",
    "uppercase_output = RunnableLambda(lambda x: x.upper())\n",
    "count_words = RunnableLambda(lambda x: f\"Word count: {len(x.split())}\")\n",
    "\n",
    "# Create the combined chain using LangChain Expression Language (LCEL)\n",
    "chain = prompt_template | model | parser | uppercase_output | count_words\n",
    "\n",
    "chain.invoke({\"language\": \"indonesia\", \"text\": \"How are you?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Pararel Chains\n",
    "\n",
    "We can create Pararel chain using class `RunnablePararel` or plain dictionary `Dict(string, Runnable)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'square': 100, 'tenth': 1.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "square = RunnableLambda(lambda x: x**2)\n",
    "tenth = RunnableLambda(lambda x : x / 10)\n",
    "\n",
    "def combine(square, tenth):\n",
    "    return {\"square\": square, \"tenth\": tenth}\n",
    "\n",
    "# using dictionary to create pararel chaining\n",
    "chain = {\"square\": square, \"tenth\": tenth} | RunnableLambda(lambda x: combine(x[\"square\"], x[\"tenth\"]))\n",
    "\n",
    "chain.invoke(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'square': 100, 'tenth': 1.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableParallel\n",
    "# Create the parallel runnable\n",
    "parallel_chain = RunnableParallel(branches={\"square\": square, \"tenth\": tenth})\n",
    "\n",
    "# Chain the parallel runnable with the combining function\n",
    "chain = parallel_chain | RunnableLambda(lambda x: combine(x[\"branches\"][\"square\"], x[\"branches\"][\"tenth\"]))\n",
    "\n",
    "chain.invoke(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Branched Chains\n",
    "\n",
    "A `RunnableBranch` is a special type of runnable that allows you to define a set of conditions and runnables to execute based on the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3 is odd'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableBranch, RunnableLambda\n",
    "\n",
    "square = RunnableLambda(lambda x: x**2)\n",
    "tenth = RunnableLambda(lambda x : x / 10)\n",
    "\n",
    "chain = RunnableBranch(\n",
    "    (lambda x: x%2 == 0, RunnableLambda(lambda x: f\"{x} is even\")),\n",
    "     RunnableLambda(lambda x: f\"{x} is odd\")\n",
    ")\n",
    "\n",
    "chain.invoke(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
