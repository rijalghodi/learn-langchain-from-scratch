{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic RAG\n",
    "\n",
    "https://api.python.langchain.com/en/latest/vectorstores/langchain_core.vectorstores.VectorStore.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Text and Document\n",
    "\n",
    "LangChain implements a Document abstraction, which is intended to represent a unit of text and associated metadata. It has two attributes:\n",
    "\n",
    "- `page_content`: a string representing the content;\n",
    "- `metadata`: a dict containing arbitrary metadata.\n",
    "\n",
    "The metadata attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual Document object often represents a chunk of a larger document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"My name is Bob\",\n",
    "        metadata={\"source\": \"my-text\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"I have one brother\",\n",
    "        metadata={\"source\": \"my-text\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"My brother is Jim\",\n",
    "        metadata={\"source\": \"my-text\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"I love playing soccer\",\n",
    "        metadata={\"source\": \"my-text\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Ronaldo is best football player\",\n",
    "        metadata={\"source\": \"random-text\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Dog is cute animal\",\n",
    "        metadata={\"source\": \"radom-text\"},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've generated six documents, containing metadata indicating two distinct \"sources\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store\n",
    "\n",
    "Vector search is a common way to store and search over unstructured data (such as unstructured text). \n",
    "\n",
    "The idea is to store numeric vectors that are associated with the text. \n",
    "\n",
    "Given a query, we can embed it as a vector of the same dimension and use vector similarity metrics to identify related data in the store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with defining embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding=OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain has several integration of vector store: First we will use Chroma to store vectors in local as sqlitea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    persist_directory=\"doc-db\" # must be specified, if not it will be crashed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also store plain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'My name is Bob',\n",
    "    'I have one brother',\n",
    "    'My brother is Jim',\n",
    "    'I love soccer',\n",
    "    'Ronaldo is the best football player',\n",
    "    'Dog is cute animal',\n",
    "]\n",
    "\n",
    "vector1 = Chroma.from_texts(\n",
    "    texts,\n",
    "    embedding=embedding,\n",
    "    persist_directory=\"text-db\" # must be specified, if not it will be crashed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Vector Store in Pinecoce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Document\\Machine_Learning_Journey\\Projects\\Langchain\\learn-langchain-from-scratch\\.venv\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore \n",
    "\n",
    "index_name = \"db\"\n",
    "\n",
    "pinecode = PineconeVectorStore.from_texts(\n",
    "    texts, embedding, index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'my-text'}, page_content='I love playing soccer'),\n",
       " Document(metadata={'source': 'my-text'}, page_content='I love playing soccer'),\n",
       " Document(metadata={'source': 'random-text'}, page_content='Ronaldo is best football player'),\n",
       " Document(metadata={'source': 'random-text'}, page_content='Ronaldo is best football player')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search('footbal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': 'my-text'}, page_content='I love playing soccer'),\n",
       "  0.29745159439630414),\n",
       " (Document(metadata={'source': 'my-text'}, page_content='I love playing soccer'),\n",
       "  0.29745159439630414),\n",
       " (Document(metadata={'source': 'random-text'}, page_content='Ronaldo is best football player'),\n",
       "  0.3417359514292345),\n",
       " (Document(metadata={'source': 'random-text'}, page_content='Ronaldo is best football player'),\n",
       "  0.3417359514292345)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search_with_score(\"Footbal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'radom-text'}, page_content='Dog is cute animal'),\n",
       " Document(metadata={'source': 'radom-text'}, page_content='Dog is cute animal'),\n",
       " Document(metadata={'source': 'my-text'}, page_content='My name is Bob'),\n",
       " Document(metadata={'source': 'my-text'}, page_content='My name is Bob')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_emb = embedding.embed_query(\"cat\")\n",
    "\n",
    "vectorstore.similarity_search_by_vector(cat_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'my-text'}, page_content='I love playing soccer'),\n",
       " Document(metadata={'source': 'my-text'}, page_content='I love playing soccer'),\n",
       " Document(metadata={'source': 'my-text'}, page_content='My name is Bob'),\n",
       " Document(metadata={'source': 'my-text'}, page_content='My name is Bob'),\n",
       " Document(metadata={'source': 'radom-text'}, page_content='Dog is cute animal')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search(\"water\", k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuids = [str(i) for i in range(len(documents))]\n",
    "\n",
    "vectorstore.add_documents(documents=documents, ids=uuids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever\n",
    "\n",
    "LangChain VectorStore objects do not subclass Runnable, and so cannot immediately be integrated into LangChain Expression Language chains.\n",
    "\n",
    "LangChain Retrievers are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous invoke and batch operations) and are designed to be incorporated in LCEL chains.\n",
    "\n",
    "We can create a simple version of this ourselves, without subclassing Retriever. If we choose what method we wish to use to retrieve documents, we can create a runnable easily. Below we will build one around the similarity_search method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'my-text'}, page_content='I love playing soccer')],\n",
       " [Document(metadata={'source': 'my-text'}, page_content='I have one brother')]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "retriever = RunnableLambda(vectorstore.similarity_search).bind(k=1) \n",
    "# .bind() pass parameter k=1 (select top result) whenever retriever is called\n",
    "\n",
    "retriever.batch([\"soccer\", \"family\"])\n",
    "# batch is using .invoke() in parallel using a thread pool executor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorstores implement an as_retriever method that will generate a Retriever, specifically a VectorStoreRetriever. These retrievers include specific search_type and search_kwargs attributes that identify what methods of the underlying vector store to call, and how to parameterize them. For instance, we can replicate the above with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'my-text'}, page_content='I love playing soccer')],\n",
       " [Document(metadata={'source': 'my-text'}, page_content='I have one brother')]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\", # alternatives: mmr, similarity_score_threshold\n",
    "    search_kwargs={\"k\": 1},\n",
    ")\n",
    "\n",
    "retriever.batch([\"soccer\", \"family\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "message = \"\"\"\n",
    "Answer this question using the provided context only.\n",
    "\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\", message)])\n",
    "\n",
    "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jim is the brother of the speaker.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"who is Jim?\")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
