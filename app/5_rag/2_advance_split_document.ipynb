{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advance Split Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with long documents is that they may exceed the context size limit of the language model (LLM). For example, GPT-3.5 has a context size of 4,096 tokens. Therefore, we need to split our document into several chunks.\n",
    "\n",
    "In LangChain, there are several strategies to split a document:\n",
    "\n",
    "1. Character Text Splitter\n",
    "2. Sentence Transformers Token Text Splitter\n",
    "3. Token Text Splitter\n",
    "4. Recursive Text Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Character Text Splitter**: \n",
    "\n",
    "- Splits text into chunks based on a specified number of characters. \n",
    "- This approach is useful for creating consistent chunk sizes regardless of the content structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_split = \"Hello there! My name is John. I have a dog, named Snowball.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello ther',\n",
       " 'ere! My na',\n",
       " 'name is Jo',\n",
       " 'John. I ha',\n",
       " 'have a dog',\n",
       " 'og, named',\n",
       " 'd Snowball',\n",
       " 'll.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Split the document into chunks\n",
    "splitter = CharacterTextSplitter(separator=\"\", chunk_size=10, chunk_overlap=2)\n",
    "splitter.split_text(text_to_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Sentence Transformers Token Text Splitter:** \n",
    "- Splits text into chunks based on sentences, ensuring that chunks end at sentence boundaries. \n",
    "- This method is ideal for maintaining semantic coherence within chunks. \n",
    "- Note that \"sentence tokens\" refer to units of text created by breaking down the text into individual sentences. Each sentence is treated as a distinct unit, preserving the natural boundaries and meaning of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Document\\Machine_Learning_Journey\\Projects\\Langchain\\learn-langchain-from-scratch\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "d:\\Document\\Machine_Learning_Journey\\Projects\\Langchain\\learn-langchain-from-scratch\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hello there! my name',\n",
       " 'my name is john.',\n",
       " 'john. i have a',\n",
       " 'have a dog, named',\n",
       " ', named snowball.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "\n",
    "splitter = SentenceTransformersTokenTextSplitter(tokens_per_chunk=5, chunk_overlap=2)\n",
    "splitter.split_text(text_to_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Token Text Splitter:** \n",
    "- Splits text into chunks based on a specified number of tokens.\n",
    "- This approach ensures that each chunk fits within the token limit of the model.\n",
    "\n",
    "- Note: remember, before it converted into embedding vector. Text will convert into numerical representation, called text encoding. There are several encoding strategy: `tiktoken`, `cl100k_base`, `gpt2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello there! My name is John. I have', ' I have a dog, named Snowball.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=2, encoding_name=\"cl100k_base\")\n",
    "splitter.split_text(text_to_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Recursive Text Splitter:** \n",
    "- Splits text recursively, starting with larger chunks and progressively breaking them down into smaller chunks if necessary. This strategy helps in balancing chunk size and content coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello there! My name is John', ' I have a dog, named Snowball']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(separators=['.'], keep_separator=False, chunk_size=10, chunk_overlap=0)\n",
    "splitter.split_text(text_to_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All splitter we've discussed can also used to split document. Just use `.split_document` instead of `.split_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './sources/sangkuriang.txt'}, page_content='Sangkuriang Story\\nThe legend tells that, long ago, there lived a beautiful woman named Dayang Sumbi, the daughter of the king of Sumbing Perbangkara. Her beautiful face made Dayang Sumbi contested by the princes.\\nAs a princess from the kingdom, Dayang Sumbi has a weaving hobby. One time, when she was busy weaving cloth, suddenly her loom fell. Instead of taking it herself, Dayang Sumbi said an oath: if the one who took the loom were a man, then she would take him as her husband, but if the one who took the loom were a woman, she would make her a sister.\\nUnexpectedly, sometime later, there came a male dog named Si Tumang, which brought Dayang Sumbi’s loom. Finally, to fulfill her oath, Dayang Sumbi married Tumang (long story short, Tumang was a god who was expelled from heaven). From that marriage, a son named Sangkuriang was born.\\nTime went on until Sangkuriang grew into a handsome boy. One day, Sangkuriang found out that his mother wanted to eat a deer’s'),\n",
       " Document(metadata={'source': './sources/sangkuriang.txt'}, page_content=' liver. Sangkuriang also hunted into the forest with Tumang. While hunting, Sangkuriang felt upset because he hadn’t successfully hunted any animals. Then, he decided to kill Tumang and gave Tumang’s heart to his mother.\\nMoments later, Dayang Sumbi found out that Sangkuriang had killed Tumang. Furiously, Dayang Sumbi hit Sangkuriang’s head and threw him out of the house.\\nYears later, Sangkuriang, who was kicked out of the house, had grown into a handsome man. While Dayang Sumbi remained young, this was because at that time Dayang Sumbi ate Tumang’s liver. One day, Sangkuriang met Dayang Sumbi again, and they fell in love with each other.\\nDayang Sumbi, who finally found out that the young man was her biological son who had been expelled for years, insisted on refusing and decided on Sangkuriang to do an impossible assignment.\\nDayang Sumbi said that if Sangkuriang wanted to marry her, he had to build a big boat in just one night. Sangkuriang also agreed.'),\n",
       " Document(metadata={'source': './sources/sangkuriang.txt'}, page_content=' With the help of the spirits, Sangkuriang almost succeeded in the task before dawn.\\nHowever, Dayang Sumbi did not remain silent. She and the women around her pounded the mortar and made it look like the dawn had come. The spirits fled, so Sangkuriang failed to finish his boat. This made Sangkuriang furious and kicked the boat upside down. The overturned boat is now known as Mount Tangkuban Perahu.\\n')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text loader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "text_loader = TextLoader('./sources/sangkuriang.txt')\n",
    "text_loader.load()\n",
    "\n",
    "document_to_split = text_loader.load()\n",
    "\n",
    "splitter = TokenTextSplitter(chunk_size=250, chunk_overlap=0, encoding_name=\"cl100k_base\")\n",
    "d = splitter.split_documents(document_to_split)\n",
    "d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
